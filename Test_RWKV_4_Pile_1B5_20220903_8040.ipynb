{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPioYiSM8omC9rO//IeQUU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bernhardkaindl/colab-notebooks/blob/main/Test_RWKV_4_Pile_1B5_20220903_8040.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on: https://colab.research.google.com/drive/1F7tZoPZaWJf1fsCmZ5tjw6sYHiFOYVWM#scrollTo=Co9eLstRwRZ_"
      ],
      "metadata": {
        "id": "l-qlgjkwKlM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oX_QF86z2tTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7277c02d-141d-4751-9796-3b5b623dbc0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RWKV-LM'...\n",
            "remote: Enumerating objects: 1696, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (131/131), done.\u001b[K\n",
            "remote: Total 1696 (delta 156), reused 200 (delta 119), pack-reused 1446\u001b[K\n",
            "Receiving objects: 100% (1696/1696), 10.73 MiB | 16.91 MiB/s, done.\n",
            "Resolving deltas: 100% (1055/1055), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BlinkDL/RWKV-LM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/RWKV-4-Pile-1B5-20220903-8040.pth -O ./RWKV-LM/RWKV-v4/500.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-tZEciz5pXL",
        "outputId": "6d01db95-4836-420f-f764-a0d5ae1e245b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-01 17:55:22--  https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/RWKV-4-Pile-1B5-20220903-8040.pth\n",
            "Resolving huggingface.co (huggingface.co)... 34.203.133.210, 35.173.225.216, 52.22.128.237, ...\n",
            "Connecting to huggingface.co (huggingface.co)|34.203.133.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/d6/95/d69583b06567422d104d5413e7926ae97bcf0d541619db6e61fe10133d91582d/4e215be3b4f86dc2f145835b47a2c432306c373cbf625375b7721bb474512bad?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-Pile-1B5-20220903-8040.pth%3B+filename%3D%22RWKV-4-Pile-1B5-20220903-8040.pth%22%3B&Expires=1680630924&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q2Lzk1L2Q2OTU4M2IwNjU2NzQyMmQxMDRkNTQxM2U3OTI2YWU5N2JjZjBkNTQxNjE5ZGI2ZTYxZmUxMDEzM2Q5MTU4MmQvNGUyMTViZTNiNGY4NmRjMmYxNDU4MzViNDdhMmM0MzIzMDZjMzczY2JmNjI1Mzc1Yjc3MjFiYjQ3NDUxMmJhZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA2MzA5MjR9fX1dfQ__&Signature=aeFBwyFbiZU3JeJ3I%7EzuQDp13qY8dyJ5xA0Obwm6425bamgg8bSkMGtEg-Q0ThYj%7ENzRAnR7WzekrWbZnScF0Vc0FB3r-FCMD2KeSHGDDEACYQBIWYQxK3f%7ErIuBvLptGFUysnbYw8QJVdbbuq-kwyazdlU4xokvK1oyl%7EF4MbGVHLQEIhaSQxQXVIYvCo6fmO8JwqpIizC%7Et7gMrNNyoVYjHdP2Yks8XjXYR-JMDuE7Cn0JnbOOBzAGn0Z4TMK5YgcLW4bkKX2a0CS0oIKjUHyBvudJLEiN5h8-3xCd0mhiNgNMhOdr1lokNUr00GqERfiwnt7ed1oE-ED8AA10zA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-04-01 17:55:23--  https://cdn-lfs.huggingface.co/repos/d6/95/d69583b06567422d104d5413e7926ae97bcf0d541619db6e61fe10133d91582d/4e215be3b4f86dc2f145835b47a2c432306c373cbf625375b7721bb474512bad?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-Pile-1B5-20220903-8040.pth%3B+filename%3D%22RWKV-4-Pile-1B5-20220903-8040.pth%22%3B&Expires=1680630924&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q2Lzk1L2Q2OTU4M2IwNjU2NzQyMmQxMDRkNTQxM2U3OTI2YWU5N2JjZjBkNTQxNjE5ZGI2ZTYxZmUxMDEzM2Q5MTU4MmQvNGUyMTViZTNiNGY4NmRjMmYxNDU4MzViNDdhMmM0MzIzMDZjMzczY2JmNjI1Mzc1Yjc3MjFiYjQ3NDUxMmJhZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODA2MzA5MjR9fX1dfQ__&Signature=aeFBwyFbiZU3JeJ3I%7EzuQDp13qY8dyJ5xA0Obwm6425bamgg8bSkMGtEg-Q0ThYj%7ENzRAnR7WzekrWbZnScF0Vc0FB3r-FCMD2KeSHGDDEACYQBIWYQxK3f%7ErIuBvLptGFUysnbYw8QJVdbbuq-kwyazdlU4xokvK1oyl%7EF4MbGVHLQEIhaSQxQXVIYvCo6fmO8JwqpIizC%7Et7gMrNNyoVYjHdP2Yks8XjXYR-JMDuE7Cn0JnbOOBzAGn0Z4TMK5YgcLW4bkKX2a0CS0oIKjUHyBvudJLEiN5h8-3xCd0mhiNgNMhOdr1lokNUr00GqERfiwnt7ed1oE-ED8AA10zA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.254.47, 13.227.254.33, 13.227.254.123, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.254.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3030279587 (2.8G) [application/zip]\n",
            "Saving to: ‘./RWKV-LM/RWKV-v4/500.pth’\n",
            "\n",
            "./RWKV-LM/RWKV-v4/5 100%[===================>]   2.82G  86.1MB/s    in 33s     \n",
            "\n",
            "2023-04-01 17:55:56 (88.0 MB/s) - ‘./RWKV-LM/RWKV-v4/500.pth’ saved [3030279587/3030279587]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.22 ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "_3R4tgtj5xPu",
        "outputId": "c54a018b-50d2-4c29-ae60-57091d0a2f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.22\n",
            "  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ninja in /usr/local/lib/python3.9/dist-packages (1.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (3.10.7)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22) (2.0.12)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.27.4\n",
            "    Uninstalling transformers-4.27.4:\n",
            "      Successfully uninstalled transformers-4.27.4\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./RWKV-LM/RWKV-v4/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFU5lwbI51O6",
        "outputId": "edbbaea8-2e9d-4d9b-fa18-2f022a11f866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RWKV-LM/RWKV-v4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################\n",
        "# The RWKV Language Model - https://github.com/BlinkDL/RWKV-LM\n",
        "########################################################################################################\n",
        "import numpy as np\n",
        "import math, os\n",
        "import time\n",
        "import types\n",
        "import copy\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from src.utils import TOKENIZER, Dataset\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
        "\n",
        "########################################################################################################\n",
        "# Step 1: set model\n",
        "# \n",
        "# Set TOKEN_MODE to 'char' or 'bpe' if the model is trained by 'train.py' from scratch.\n",
        "#\n",
        "# Set TOKEN_MODE to 'pile' if you want to test pre-trained pile models.\n",
        "########################################################################################################\n",
        "\n",
        "TOKEN_MODE = 'pile' # char / bpe / pile\n",
        "\n",
        "n_layer = 6\n",
        "n_embd = 512\n",
        "ctx_len = 10024\n",
        "\n",
        "if TOKEN_MODE == 'char':\n",
        "    MODEL_NAME = 'trained-500'  # your trained model\n",
        "    WORD_NAME = 'vocab'         # the .json vocab (generated by train.py)\n",
        "    # set UNKNOWN_CHAR to the rarest token in your vocab.json, and all unknown tokens in your prompt will be denoted by it\n",
        "    UNKNOWN_CHAR = ' '          # here we just set it to ' ' for simplicity\n",
        "\n",
        "elif TOKEN_MODE == 'bpe':\n",
        "    MODEL_NAME = 'trained-500'  # your trained model\n",
        "    WORD_NAME = ['model-vocab.json', 'model-merges.txt'] # [vocab, merge] for your BPE model\n",
        "    UNKNOWN_CHAR = None\n",
        "\n",
        "elif TOKEN_MODE == 'pile':\n",
        "    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']\n",
        "    UNKNOWN_CHAR = None\n",
        "\n",
        "    #---> you can set MODEL_NAME to your fine-tuned model <---\n",
        "\n",
        "    MODEL_NAME = '500'\n",
        "   \n",
        "    # for 3b\n",
        "    #n_layer = 32\n",
        "    #n_embd = 2560\n",
        "    #ctx_len = 10024\n",
        "\n",
        "    # for 1b5'\n",
        "    n_layer = 24\n",
        "    n_embd = 2048\n",
        "    ctx_len = 1024\n",
        "\n",
        "os.environ['RWKV_FLOAT_MODE'] = 'bf16'  # 'bf16' / 'fp16' / 'fp32' (note: only using fp32 at this moment)\n",
        "os.environ['RWKV_RUN_DEVICE'] = 'cuda'   # 'cpu' (already very fast) or 'cuda'\n",
        "model_type = 'RWKV' # 'RWKV' or 'RWKV-ffnPre'\n",
        "\n",
        "########################################################################################################\n",
        "# Step 2: set prompt & sampling stuffs\n",
        "########################################################################################################\n",
        "\n",
        "# context = 'A'\n",
        "# context = \"\\nIn the\"\n",
        "# context = '\\nSugar:'\n",
        "\n",
        "NUM_TRIALS = 5\n",
        "LENGTH_PER_TRIAL = 3330\n",
        "\n",
        "DEBUG_DEBUG = False  # True False --> show softmax output\n",
        "\n",
        "########################################################################################################\n",
        "\n",
        "print(f'Loading {MODEL_NAME}...')\n",
        "from src.model_run import RWKV_RNN\n",
        "model = RWKV_RNN(MODEL_NAME, os.environ['RWKV_RUN_DEVICE'], model_type, n_layer, n_embd, ctx_len)\n",
        "tokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htNvWfOI6Ngr",
        "outputId": "9f635f75-3a2b-4a32-e6e3-5bd06e1c97b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 500...\n",
            "\n",
            "RWKV_HEAD_QK_DIM 0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py39_cu116 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py39_cu116/wkv/build.ninja...\n",
            "Building extension module wkv...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module wkv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUykZE6V90wu",
        "outputId": "f4554249-5ef5-4b5d-f7ee-7d786906cbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr  1 18:50:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P0    28W /  70W |   9739MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"You are an AI running a house.\\ngiven the following commands: volumeUp(int amount),volumeDown(int amount),setVolume(int percent),setLights([r,g,b]),playSong(string url)\\nand the given instruction 'Please make the room romantic'\\nList the commands, and the parameters they should have, that should be done to fullfil the command\\nGive the commands in the format [command(parameter)]\\n\\nTask: list the commands and a reasonable value for the parameter\\nResponse:\"\n",
        "\n",
        "TEMPERATURE = 0.9\n",
        "top_p = 0.8\n",
        "top_p_newline = 0.9 # only used in TOKEN_MODE = char\n",
        "\n",
        "if tokenizer.charMode:\n",
        "    context = tokenizer.refine_context(context)\n",
        "    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n",
        "else:\n",
        "    ctx = tokenizer.tokenizer.encode(context)\n",
        "src_len = len(ctx)\n",
        "src_ctx = ctx.copy()\n",
        "\n",
        "print('\\nYour prompt has ' + str(src_len) + ' tokens.')\n",
        "print('\\n--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. Use GPT to build the hidden state for better speed. <--\\n')\n",
        "\n",
        "for TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n",
        "    t_begin = time.time_ns()\n",
        "    print(('-' * 30) + context, end='')\n",
        "    ctx = src_ctx.copy()\n",
        "    model.clear()\n",
        "    if TRIAL == 0:\n",
        "        init_state = types.SimpleNamespace()\n",
        "        for i in range(src_len):\n",
        "            x = ctx[:i+1]\n",
        "            if i == src_len - 1:\n",
        "                init_state.out = model.run(x)\n",
        "            else:\n",
        "                model.run(x)\n",
        "        model.save(init_state)\n",
        "    else:\n",
        "        model.load(init_state)\n",
        "\n",
        "    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n",
        "        x = ctx[:i+1]\n",
        "        x = x[-ctx_len:]\n",
        "\n",
        "        if i == src_len:\n",
        "            out = copy.deepcopy(init_state.out)\n",
        "        else:\n",
        "            out = model.run(x)\n",
        "        if DEBUG_DEBUG:\n",
        "            print('model', np.array(x), '==>', np.array(\n",
        "                out), np.max(out), np.min(out))\n",
        "\n",
        "        if TOKEN_MODE == 'pile':\n",
        "            out[0] = -999999999  # disable <|endoftext|>\n",
        "\n",
        "        char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n",
        "                                       top_p_usual=top_p, top_p_newline=top_p_newline)\n",
        "        char = char.item()\n",
        "        if tokenizer.charMode:\n",
        "            print(tokenizer.itos[int(char)], end='', flush=True)\n",
        "        else:\n",
        "            print(tokenizer.tokenizer.decode(int(char)), end='', flush=True)\n",
        "        ctx += [char]\n",
        "\n",
        "    t_end = time.time_ns()\n",
        "    print(\"\\n----------\", round((t_end - t_begin) / (10 ** 9), 2), end='s ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K0AIYjfT-FfS",
        "outputId": "5e751968-2a0a-42e0-9c63-ebacc03fa5b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Your prompt has 110 tokens.\n",
            "\n",
            "--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. Use GPT to build the hidden state for better speed. <--\n",
            "\n",
            "------------------------------You are an AI running a house.\n",
            "given the following commands: volumeUp(int amount),volumeDown(int amount),setVolume(int percent),setLights([r,g,b]),playSong(string url)\n",
            "and the given instruction 'Please make the room romantic'\n",
            "List the commands, and the parameters they should have, that should be done to fullfil the command\n",
            "Give the commands in the format [command(parameter)]\n",
            "\n",
            "Task: list the commands and a reasonable value for the parameter\n",
            "Response:\n",
            "volumesound(int volume)\n",
            "makes the room sounds louder\n",
            "setvolume(int volume)\n",
            "makes the room sound brighter\n",
            "setLights(string name)\n",
            "makes the room have a light (or no lights)\n",
            "setVolume(int volume)\n",
            "makes the room volume louder or quieter\n",
            "setLights(string name)\n",
            "makes the room have lights or no lights\n",
            "setVolume(int volume)\n",
            "makes the room have a reasonable value for the parameter\n",
            "make the room an AI running a house\n",
            "tasks: make the room sound louder (or no louder)\n",
            "Please make the room an AI running a house.\n",
            "Task: make the house have a reasonable value for the parameter\n",
            "give the room a reasonable value for the parameter\n",
            "make the room a reasonable value for the parameter\n",
            "make the room an AI running a task\n",
            "\n",
            "The task is really simple, just running a function to play music and make the volume sound higher/lower depending on what I tell it.\n",
            "the command will have parameters for the volume that needs to be played, and the volume should be given as a percentage (e.g. 50%).\n",
            "I can get the volume up by giving it 100%, and then playing the sound for a few seconds. I have a function for that.\n",
            "However, it doesn't seem to be able to change the sound. So I tried making the command play a music track with \"music track \" + str(volume)+\"%\" and it doesn't work.\n",
            "Any help would be greatly appreciated!\n",
            "\n",
            "A:\n",
            "\n",
            "Found it.\n",
            "in my player class, I declared it to have a track variable.  So the command for setting volume is:\n",
            "player.setVolume(int volume)\n",
            "\n",
            "When it receives a command to set volume, it calls the command like so:\n",
            "player.setVolume(volume)\n",
            "\n",
            "So, I changed my player class to this:\n",
            "import random\n",
            "\n",
            "volumeUp = 0\n",
            "volumeDown = 0\n",
            "\n",
            "class player:\n",
            "    def __init__(self,name,volume):\n",
            "        self.name = name\n",
            "        self.volume = volume\n",
            "        self.volumeUp = 0\n",
            "        self.volumeDown = 0\n",
            "\n",
            "    def setVolume(self,int volume):\n",
            "        self.volume = int(volume)\n",
            "        self.volumeUp = int(volumeUp)\n",
            "        self.volumeDown = int(volumeDown)\n",
            "\n",
            "    def setLights(self,int light):\n",
            "        self.lights = light\n",
            "        self.lights = random.randint(0,5)\n",
            "\n",
            "    def setVolume(self,int volume):\n",
            "        self.volume = int(volume)\n",
            "        self.volume = random.randint(0,50)\n",
            "\n",
            "    def setLights(self,int lights):\n",
            "        self.lights = lights\n",
            "        self.lights = random.randint(0,50)\n",
            "\n",
            "    def makeCommand(self,command, response, varName, paramName):\n",
            "        try:\n",
            "            command(paramName)\n",
            "        except Exception as e:\n",
            "            print e\n",
            "\n",
            "    def setVolume(self,int volume):\n",
            "        self.volume = int(volume)\n",
            "\n",
            "    def setVolume(self,int volume):\n",
            "        self.volume = int(volume)\n",
            "\n",
            "    def setVolume(int volume):\n",
            "        self.volume = int(volume)\n",
            "\n",
            "    def makeCommand(self,command,paramName,varName,paramValue):\n",
            "        command(paramName)\n",
            "        command(paramValue)\n",
            "\n",
            "class player:\n",
            "    def __init__(self,name,volume,volumeUp,volumeDown):\n",
            "        self.name = name\n",
            "        self.volume = volume\n",
            "        self.volumeUp = volumeUp\n",
            "        self.volumeDown = volumeDown\n",
            "\n",
            "    def makeCommand(self,command,paramName,varName,paramValue):\n",
            "        command(paramName)\n",
            "        command(paramValue)\n",
            "\n",
            "    def makeCommand(self,command,paramName,varName,paramValue):\n",
            "        command(paramName)\n",
            "        command(paramValue)\n",
            "\n",
            "    def makeCommand(self,command,paramName,varName,paramValue):\n",
            "        command(paramName)\n",
            "        command(paramValue)\n",
            "\n",
            "    def makeCommand(self,command,paramName,varName,paramValue):\n",
            "        command(paramName)\n",
            "        command(paramValue)\n",
            "\n",
            "    def playSong(self,url):\n",
            "        try:\n",
            "            command = player.makeCommand(url)\n",
            "            command.playSong(self.name,self.volume,self.volumeUp,self.volumeDown)\n",
            "        except Exception as e:\n",
            "            print e\n",
            "\n",
            "class playerClass:\n",
            "    def __init__(self):\n",
            "        self.room = []\n",
            "        self.player = player()\n",
            "        self.room.makeCommand(\"varName\", [paramName,varValue])\n",
            "        self.room.makeCommand(\"varName\", [paramValue,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", [paramName,paramValue])\n",
            "        self.room.makeCommand(\"varName\", ["
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-889ef2cc1de1>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m999999999\u001b[0m  \u001b[0;31m# disable <|endoftext|>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n\u001b[0m\u001b[1;32m     51\u001b[0m                                        top_p_usual=top_p, top_p_newline=top_p_newline)\n\u001b[1;32m     52\u001b[0m         \u001b[0mchar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RWKV-LM/RWKV-v4/src/utils.py\u001b[0m in \u001b[0;36msample_logits\u001b[0;34m(self, out, x, ctx_len, temperature, top_p_usual, top_p_newline)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mlastChar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharMode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}